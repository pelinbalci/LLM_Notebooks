{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Bli4FAZ8akD+qq2uPLnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pelinbalci/LLM_Notebooks/blob/main/Basic_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5rH0jyK375J",
        "outputId": "1833feb0-a5b9-4665-b828-44240232a8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PIP_DISABLE_PIP_VERSION_CHECK=True\n",
            "env: PIP_ROOT_USER_ACTION=ignore\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%env PIP_DISABLE_PIP_VERSION_CHECK True\n",
        "%env PIP_ROOT_USER_ACTION ignore\n",
        "\n",
        "%pip install -q -U torch==2.0.1 bitsandbytes==0.39.1 --root-user-action=ignore\n",
        "%pip install -q -U datasets py7zr einops tensorboardX --root-user-action=ignore\n",
        "%pip install -q -U git+https://github.com/huggingface/transformers.git@850cf4af0ce281d2c3e7ebfc12e0bc24a9c40714 --root-user-action=ignore\n",
        "%pip install -q -U git+https://github.com/huggingface/peft.git@e2b8e3260d3eeb736edf21a2424e89fe3ecf429d --root-user-action=ignore\n",
        "%pip install -q -U git+https://github.com/huggingface/accelerate.git@b76409ba05e6fa7dfc59d50eee1734672126fdba --root-user-action=ignore\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Inference with T5\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "\n",
        "# Use AutoModelForseq2Seq for model and AutoTokenizer for tokenizer.\n",
        "model_name = 'google/flan-t5-base'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "\n",
        "# Configurations\n",
        "generation_config = GenerationConfig(max_new_tokens=50,\n",
        "                                     do_sample=True,\n",
        "                                     temperature=0.7)\n",
        "\n",
        "# Sample data\n",
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "example_index = [40]  # select an example\n",
        "dialogue = dataset['test'][example_index]['dialogue']  # get the dialogue\n",
        "summary = dataset['test'][example_index]['summary'] # get the human summary\n",
        "\n",
        "# Encode input\n",
        "inputs_encoded = tokenizer(dialogue, return_tensors='pt')\n",
        "# Model Output\n",
        "model_output = model.generate(inputs_encoded[\"input_ids\"], generation_config=generation_config)[0]\n",
        "\n",
        "# Decode the output\n",
        "zero_output = tokenizer.decode(model_output, skip_special_tokens=True)\n",
        "\n",
        "print(\"Dialogue:\")\n",
        "print(dialogue)\n",
        "print(\"-------------------------\")\n",
        "print(\"summary:\")\n",
        "print(summary)\n",
        "print(\"-------------------------\")\n",
        "print(\"inputs_encoded:\")\n",
        "print(inputs_encoded)\n",
        "print(\"-------------------------\")\n",
        "print(\"model_output:\")\n",
        "print(model_output)\n",
        "print(\"-------------------------\")\n",
        "print(\"zero_output:\")\n",
        "print(zero_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDDmUyR7Mpqr",
        "outputId": "62eae807-a8af-409e-d48a-f202cad4183b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue:\n",
            "[\"#Person1#: What time is it, Tom?\\n#Person2#: Just a minute. It's ten to nine by my watch.\\n#Person1#: Is it? I had no idea it was so late. I must be off now.\\n#Person2#: What's the hurry?\\n#Person1#: I must catch the nine-thirty train.\\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\"]\n",
            "-------------------------\n",
            "summary:\n",
            "['#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.']\n",
            "-------------------------\n",
            "inputs_encoded:\n",
            "{'input_ids': tensor([[ 1713,   345, 13515,   536,  4663,    10,   363,    97,    19,    34,\n",
            "             6,  3059,    58,  1713,   345, 13515,   357,  4663,    10,  1142,\n",
            "             3,     9,  1962,     5,    94,    31,     7,     3,   324,    12,\n",
            "          4169,    57,    82,  1605,     5,  1713,   345, 13515,   536,  4663,\n",
            "            10,    27,     7,    34,    58,    27,   141,   150,   800,    34,\n",
            "            47,    78,  1480,     5,    27,   398,    36,   326,   230,     5,\n",
            "          1713,   345, 13515,   357,  4663,    10,   363,    31,     7,     8,\n",
            "         23601,    58,  1713,   345, 13515,   536,  4663,    10,    27,   398,\n",
            "          3579,     8,  4169,    18,    17,  9288,    17,    63,  2412,     5,\n",
            "          1713,   345, 13515,   357,  4663,    10,   148,    31,   162,  2500,\n",
            "            13,    97,   780,     5,    37, 14421,  2478,    19,   182,   885,\n",
            "             5,    94,   751,    31,    17,   240,    72,   145,  6786,   676,\n",
            "            12,   129,   132,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1]])}\n",
            "-------------------------\n",
            "model_output:\n",
            "tensor([    0,  3059,   845,     3,    88,    56,    36,    44,     8, 14421,\n",
            "         2478,    16,   705,   145,   460,   676,     6,    78,     3,    88,\n",
            "         4054,     3,    88,    54,  3579,     8,  4169,    18,    17,  9288,\n",
            "           17,    63,  2412,     5,     1])\n",
            "-------------------------\n",
            "zero_output:\n",
            "Tom says he will be at the railway station in less than 20 minutes, so he knows he can catch the nine-thirty train.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic inference 2:\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "base_model.to(device)\n",
        "\n",
        "test_text = \"How does gender equality benefit society?\"\n",
        "max_input_tokens = 1000\n",
        "max_output_tokens=100\n",
        "# Tokenize\n",
        "input_ids = tokenizer.encode(\n",
        "      test_text,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=max_input_tokens\n",
        ")\n",
        "\n",
        "# Generate\n",
        "generated_tokens_with_prompt = base_model.generate(input_ids=input_ids.to(device), max_length=max_output_tokens)\n",
        "\n",
        "# Decode\n",
        "generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "# Strip the prompt\n",
        "generated_text_answer = generated_text_with_prompt[0][len(test_text):]\n",
        "\n",
        "\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(\"Model's answer: \")\n",
        "print(generated_text_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFVYhKyUMvDc",
        "outputId": "40e50f08-9af8-4192-e75e-2aba24773590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): How does gender equality benefit society?\n",
            "Model's answer: \n",
            "\n",
            "\n",
            "A:\n",
            "\n",
            "The only way to get rid of this is to use the \"gender equality\" option.\n",
            "\n",
            "A:\n",
            "\n",
            "The only way to get rid of this is to use the \"gender equality\" option.\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the \"gender equality\" option.\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the \"gender equality\" option.\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the \"gender equality\n"
          ]
        }
      ]
    }
  ]
}