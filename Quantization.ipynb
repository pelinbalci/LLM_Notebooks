{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pelinbalci/LLM_Notebooks/blob/main/Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize - DeQuantize\n",
        "\n",
        "Original Codes for this part: https://github.com/hkproj/quantization-notes/blob/main/quantization_from_scratch.ipynb"
      ],
      "metadata": {
        "id": "-UmwRn1ErQCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Suppress scientific notation\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Generate randomly distributed parameters\n",
        "params = np.random.uniform(low=-50, high=150, size=20)\n",
        "\n",
        "# Make sure important values are at the beginning for better debugging\n",
        "params[0] = params.max() + 1\n",
        "params[1] = params.min() - 1\n",
        "params[2] = 0\n",
        "\n",
        "# Round each number to the second decimal place\n",
        "params = np.round(params, 2)\n",
        "\n",
        "# Print the parameters\n",
        "print(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Rg_l6erPs3",
        "outputId": "754254ef-a23f-4404-e5d8-4e3f2379752a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[147.16 -46.83   0.    26.65 135.02  60.88   1.46 124.1  142.15  53.96\n",
            " -45.83  31.05  -7.78 146.16 131.59 -24.1   27.18 116.29 122.6  -44.58]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clamp(params_q: np.array, lower_bound: int, upper_bound: int) -> np.array:\n",
        "    params_q[params_q < lower_bound] = lower_bound\n",
        "    params_q[params_q > upper_bound] = upper_bound\n",
        "    return params_q\n",
        "\n",
        "def asymmetric_quantization(params: np.array, bits: int) -> tuple[np.array, float, int]:\n",
        "    # Calculate the scale and zero point\n",
        "    max_param = np.max(params)\n",
        "    min_param = np.min(params)\n",
        "    scale = (max_param - min_param) / (2**bits-1)\n",
        "    zero = -1*np.round(min_param / scale)\n",
        "    lower_bound, upper_bound = 0, 2**bits-1\n",
        "    # Quantize the parameters\n",
        "    quantized = clamp(np.round(params / scale + zero), lower_bound, upper_bound).astype(np.int32)\n",
        "    return quantized, scale, zero\n",
        "\n",
        "def asymmetric_dequantize(params_q: np.array, scale: float, zero: int) -> np.array:\n",
        "    return (params_q - zero) * scale\n",
        "\n",
        "def symmetric_dequantize(params_q: np.array, scale: float) -> np.array:\n",
        "    return params_q * scale\n",
        "\n",
        "def symmetric_quantization(params: np.array, bits: int) -> tuple[np.array, float]:\n",
        "    # Calculate the scale\n",
        "    max_param_abs = np.max(np.abs(params))\n",
        "    scale = max_param_abs / (2**(bits-1)-1)\n",
        "    lower_bound = -2**(bits-1)\n",
        "    upper_bound = 2**(bits-1)-1\n",
        "    # Quantize the parameters\n",
        "    quantized = clamp(np.round(params / scale), lower_bound, upper_bound).astype(np.int32)\n",
        "    return quantized, scale\n",
        "\n",
        "def quantization_error(params: np.array, params_q: np.array):\n",
        "    # calculate the MSE\n",
        "    return np.mean((params - params_q)**2)\n",
        "\n",
        "(asymmetric_q, asymmetric_scale, asymmetric_zero) = asymmetric_quantization(params, 8)\n",
        "(symmetric_q, symmetric_scale) = symmetric_quantization(params, 8)\n",
        "\n",
        "print(f'Original:')\n",
        "print(np.round(params, 2))\n",
        "print('')\n",
        "print(f'Asymmetric scale: {asymmetric_scale}, zero: {asymmetric_zero}')\n",
        "print(asymmetric_q)\n",
        "print('')\n",
        "print(f'Symmetric scale: {symmetric_scale}')\n",
        "print(symmetric_q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFQztHE7rPlQ",
        "outputId": "d878001d-c7de-4b12-f041-71e08483a83f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "[147.16 -46.83   0.    26.65 135.02  60.88   1.46 124.1  142.15  53.96\n",
            " -45.83  31.05  -7.78 146.16 131.59 -24.1   27.18 116.29 122.6  -44.58]\n",
            "\n",
            "Asymmetric scale: 0.7607450980392158, zero: 62.0\n",
            "[255   0  62  97 239 142  64 225 249 133   2 103  52 254 235  30  98 215\n",
            " 223   3]\n",
            "\n",
            "Symmetric scale: 1.158740157480315\n",
            "[127 -40   0  23 117  53   1 107 123  47 -40  27  -7 126 114 -21  23 100\n",
            " 106 -38]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dequantize the parameters back to 32 bits\n",
        "params_deq_asymmetric = asymmetric_dequantize(asymmetric_q, asymmetric_scale, asymmetric_zero)\n",
        "params_deq_symmetric = symmetric_dequantize(symmetric_q, symmetric_scale)\n",
        "\n",
        "print(f'Original:')\n",
        "print(np.round(params, 2))\n",
        "print('')\n",
        "print(f'Dequantize Asymmetric:')\n",
        "print(np.round(params_deq_asymmetric,2))\n",
        "print('')\n",
        "print(f'Dequantize Symmetric:')\n",
        "print(np.round(params_deq_symmetric, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLSHZe9GrPRz",
        "outputId": "b0f221ad-1a83-4691-e3d6-21b8b83d8c4d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "[147.16 -46.83   0.    26.65 135.02  60.88   1.46 124.1  142.15  53.96\n",
            " -45.83  31.05  -7.78 146.16 131.59 -24.1   27.18 116.29 122.6  -44.58]\n",
            "\n",
            "Dequantize Asymmetric:\n",
            "[146.82 -47.17   0.    26.63 134.65  60.86   1.52 124.   142.26  54.01\n",
            " -45.64  31.19  -7.61 146.06 131.61 -24.34  27.39 116.39 122.48 -44.88]\n",
            "\n",
            "Dequantize Symmetric:\n",
            "[147.16 -46.35   0.    26.65 135.57  61.41   1.16 123.99 142.53  54.46\n",
            " -46.35  31.29  -8.11 146.   132.1  -24.33  26.65 115.87 122.83 -44.03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the quantization error\n",
        "print(f'{\"Asymmetric error: \":>20}{np.round(quantization_error(params, params_deq_asymmetric), 2)}')\n",
        "print(f'{\"Symmetric error: \":>20}{np.round(quantization_error(params, params_deq_symmetric), 2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm23CLDUr_EY",
        "outputId": "c69b9882-fe53-4b35-9564-695620a41827"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Asymmetric error: 0.04\n",
            "   Symmetric error: 0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bge68_mqNalN"
      },
      "source": [
        "# Quantization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz64HNKpA7Wj"
      },
      "source": [
        "## Quantization with Data Types or Downcasting - PyTorch\n",
        "\n",
        "The model's parameters are saved in a more compact data type (bfloat16).\n",
        "During inference, the model performs its calculations in this data type, and its activations are in this data type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oooLv-9QKZDB",
        "outputId": "be320c6c-6de8-407e-ab3b-cbc41b3b15a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DummyModel(\n",
            "  (token_embedding): Embedding(2, 2)\n",
            "  (linear_1): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (layernorm_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
            "  (linear_2): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (layernorm_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
            "  (head): Linear(in_features=2, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DummyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DummyModel, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(2, 2)\n",
        "        self.linear_1 = nn.Linear(2, 2)\n",
        "        self.layernorm_1 = nn.LayerNorm(2)\n",
        "        self.linear_2 = nn.Linear(2, 2)\n",
        "        self.layernorm_2 = nn.LayerNorm(2)\n",
        "        self.head = nn.Linear(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.layernorm_1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.layernorm_2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "# Testing the model\n",
        "model = DummyModel()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhqbC6hdKcs-"
      },
      "outputs": [],
      "source": [
        "def print_param_dtype(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name} is loaded in {param.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lNtxe5SKgpw",
        "outputId": "e8c1cc4b-1801-48a2-af2d-91f999602fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token_embedding.weight is loaded in torch.float32\n",
            "linear_1.weight is loaded in torch.float32\n",
            "linear_1.bias is loaded in torch.float32\n",
            "layernorm_1.weight is loaded in torch.float32\n",
            "layernorm_1.bias is loaded in torch.float32\n",
            "linear_2.weight is loaded in torch.float32\n",
            "linear_2.bias is loaded in torch.float32\n",
            "layernorm_2.weight is loaded in torch.float32\n",
            "layernorm_2.bias is loaded in torch.float32\n",
            "head.weight is loaded in torch.float32\n",
            "head.bias is loaded in torch.float32\n"
          ]
        }
      ],
      "source": [
        "print_param_dtype(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrFzMQvPKjaH",
        "outputId": "399eb46b-998d-4315-d729-3ce92f5d02b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token_embedding.weight is loaded in torch.float16\n",
            "linear_1.weight is loaded in torch.float16\n",
            "linear_1.bias is loaded in torch.float16\n",
            "layernorm_1.weight is loaded in torch.float16\n",
            "layernorm_1.bias is loaded in torch.float16\n",
            "linear_2.weight is loaded in torch.float16\n",
            "linear_2.bias is loaded in torch.float16\n",
            "layernorm_2.weight is loaded in torch.float16\n",
            "layernorm_2.bias is loaded in torch.float16\n",
            "head.weight is loaded in torch.float16\n",
            "head.bias is loaded in torch.float16\n"
          ]
        }
      ],
      "source": [
        "# float 16\n",
        "model_fp16 = DummyModel().half()\n",
        "print_param_dtype(model_fp16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYaUjHonKpzE",
        "outputId": "1fb6b7c2-4ce9-4aa1-d920-343b92bcef2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.4875, -0.6201],\n",
              "         [-0.4875, -0.6201]],\n",
              "\n",
              "        [[-0.4875, -0.6201],\n",
              "         [-0.4875, -0.6201]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dummy_input = torch.LongTensor([[1, 0], [0, 1]])\n",
        "\n",
        "# inference using float32 model\n",
        "logits_fp32 = model(dummy_input)\n",
        "\n",
        "logits_fp32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBFywyNRK02p",
        "outputId": "73c56a96-7bd9-4b99-c638-5333355533d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[91m RuntimeError :  \"LayerNormKernelImpl\" not implemented for 'Half' \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# inference using float16 model\n",
        "try:\n",
        "    logits_fp16 = model_fp16(dummy_input)\n",
        "except Exception as error:\n",
        "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D07umDhKLyKi",
        "outputId": "248f0263-e8ef-40d4-8dd4-da3ae665d3a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token_embedding.weight is loaded in torch.bfloat16\n",
            "linear_1.weight is loaded in torch.bfloat16\n",
            "linear_1.bias is loaded in torch.bfloat16\n",
            "layernorm_1.weight is loaded in torch.bfloat16\n",
            "layernorm_1.bias is loaded in torch.bfloat16\n",
            "linear_2.weight is loaded in torch.bfloat16\n",
            "linear_2.bias is loaded in torch.bfloat16\n",
            "layernorm_2.weight is loaded in torch.bfloat16\n",
            "layernorm_2.bias is loaded in torch.bfloat16\n",
            "head.weight is loaded in torch.bfloat16\n",
            "head.bias is loaded in torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "model_bf16 = deepcopy(model)\n",
        "model_bf16 = model_bf16.to(torch.bfloat16)\n",
        "print_param_dtype(model_bf16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-I1THQOL669",
        "outputId": "8a504d33-657d-4720-c4dd-0c6967103112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4902, -0.6250],\n",
            "         [-0.4902, -0.6250]],\n",
            "\n",
            "        [[-0.4902, -0.6250],\n",
            "         [-0.4902, -0.6250]]], dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# inference using float16 model\n",
        "try:\n",
        "    logits_bf16 = model_bf16(dummy_input)\n",
        "    print(logits_bf16)\n",
        "except Exception as error:\n",
        "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmoGoAG5ME7A",
        "outputId": "695cf9ea-c6f9-4d59-8078-9a8772609a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean diff: 0.003837764263153076 | Max diff: 0.004937291145324707\n"
          ]
        }
      ],
      "source": [
        "mean_diff = torch.abs(logits_bf16 - logits_fp32).mean().item()\n",
        "max_diff = torch.abs(logits_bf16 - logits_fp32).max().item()\n",
        "\n",
        "print(f\"Mean diff: {mean_diff} | Max diff: {max_diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REvVynrIA_K2"
      },
      "source": [
        "## Reduced Float - TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STsQyTLVBBQ4",
        "outputId": "2cc730a0-1b70-4b56-d7f1-ec8970bf3a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 201474 (787.01 KB)\n",
            "Trainable params: 201474 (787.01 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "# Define a simple sequential model\n",
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    keras.layers.Dense(256, activation='relu', input_shape=(784,)),\n",
        "    keras.layers.Dense(2)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Create a basic model instance\n",
        "model = create_model()\n",
        "\n",
        "# Display the model's architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5dCjGrsF2_F",
        "outputId": "84425b8c-c4d6-4fc1-d223-6032aee95851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF_model_size in KB 802.390625\n"
          ]
        }
      ],
      "source": [
        "# save model - first way\n",
        "model.save('model.h5')\n",
        "\n",
        "# save model - second way\n",
        "saved_model_path = 'saved_model/1'\n",
        "tf.saved_model.save(model, saved_model_path)\n",
        "\n",
        "# Get file size in bytes for a given model - first way\n",
        "TF_model_size = os.path.getsize('model.h5') /float(2**10)\n",
        "print('TF_model_size in KB', TF_model_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the layers of the model\n",
        "for layer in model.layers:\n",
        "    # Get the weights of the layer\n",
        "    weights = layer.get_weights()\n",
        "    print(len(weights[0]))\n",
        "    print(weights[0].dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu4qujcbJr5W",
        "outputId": "dceb46c1-6ad8-4790-8d1c-d67b36d07996"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n",
            "float32\n",
            "256\n",
            "float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTY-Dd2DGE1t",
        "outputId": "6091f16c-3609-4550-faa7-47a73d326f94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "394.67578125"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "converter_1 = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "converter_1.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter_1.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = converter_1.convert()\n",
        "\n",
        "open(\"converted_model.tflite\", \"wb\").write(tflite_model) /float(2**10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get tensor details\n",
        "tensor_details = interpreter.get_tensor_details()\n",
        "\n",
        "# Print data types of parameters\n",
        "for tensor in tensor_details:\n",
        "    if tensor['name'] != 'constants':\n",
        "        print('Name:', tensor['name'])\n",
        "        print('Data type:', tensor['dtype'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh_9sNkmLARe",
        "outputId": "ffdf405c-8bee-41e2-ce14-79559f9747da"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: serving_default_dense_10_input:0\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_5/dense_10/MatMul\n",
            "Data type: <class 'numpy.float16'>\n",
            "Name: sequential_5/dense_11/MatMul\n",
            "Data type: <class 'numpy.float16'>\n",
            "Name: sequential_5/dense_10/MatMul2\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_5/dense_11/MatMul1\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_5/dense_10/MatMul;sequential_5/dense_10/Relu;sequential_5/dense_10/BiasAdd\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: StatefulPartitionedCall:0\n",
            "Data type: <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_details"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6lahtsNPOaB",
        "outputId": "36d3206b-c8af-42ba-a886-2422bcc3049d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'serving_default_dense_10_input:0',\n",
              "  'index': 0,\n",
              "  'shape': array([  1, 784], dtype=int32),\n",
              "  'shape_signature': array([ -1, 784], dtype=int32),\n",
              "  'dtype': numpy.float32,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}},\n",
              " {'name': 'sequential_5/dense_10/MatMul',\n",
              "  'index': 1,\n",
              "  'shape': array([256, 784], dtype=int32),\n",
              "  'shape_signature': array([256, 784], dtype=int32),\n",
              "  'dtype': numpy.float16,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}},\n",
              " {'name': 'sequential_5/dense_11/MatMul',\n",
              "  'index': 2,\n",
              "  'shape': array([  2, 256], dtype=int32),\n",
              "  'shape_signature': array([  2, 256], dtype=int32),\n",
              "  'dtype': numpy.float16,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}},\n",
              " {'name': 'sequential_5/dense_10/MatMul2',\n",
              "  'index': 3,\n",
              "  'shape': array([256, 784], dtype=int32),\n",
              "  'shape_signature': array([256, 784], dtype=int32),\n",
              "  'dtype': numpy.float32,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}},\n",
              " {'name': 'sequential_5/dense_11/MatMul1',\n",
              "  'index': 4,\n",
              "  'shape': array([  2, 256], dtype=int32),\n",
              "  'shape_signature': array([  2, 256], dtype=int32),\n",
              "  'dtype': numpy.float32,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}},\n",
              " {'name': 'sequential_5/dense_10/MatMul;sequential_5/dense_10/Relu;sequential_5/dense_10/BiasAdd',\n",
              "  'index': 5,\n",
              "  'shape': array([  1, 256], dtype=int32),\n",
              "  'shape_signature': array([ -1, 256], dtype=int32),\n",
              "  'dtype': numpy.float32,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}},\n",
              " {'name': 'StatefulPartitionedCall:0',\n",
              "  'index': 6,\n",
              "  'shape': array([1, 2], dtype=int32),\n",
              "  'shape_signature': array([-1,  2], dtype=int32),\n",
              "  'dtype': numpy.float32,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the input and output details\n",
        "for input_detail in interpreter.get_input_details():\n",
        "    print(\"Input:\", input_detail[\"name\"], input_detail[\"dtype\"])\n",
        "\n",
        "for output_detail in interpreter.get_output_details():\n",
        "    print(\"Output:\", output_detail[\"name\"], output_detail[\"dtype\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R_agp8LOzd1",
        "outputId": "f1f6e9b5-d714-479f-e9d5-0dcd5691e7f1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: serving_default_dense_10_input:0 <class 'numpy.float32'>\n",
            "Output: StatefulPartitionedCall:0 <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Quantization - TensorFlow"
      ],
      "metadata": {
        "id": "8K079pFgSMC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter_2 = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "converter_2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model_2 = converter_2.convert()\n",
        "\n",
        "open(\"converted_hybrid_model.tflite\", \"wb\").write(tflite_model_2) /float(2**10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqEpD3QuSLqY",
        "outputId": "7058f971-2ea8-491d-e192-eb2b2ad53d8a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199.3671875"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"converted_hybrid_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get tensor details\n",
        "tensor_details = interpreter.get_tensor_details()\n",
        "\n",
        "# Print data types of parameters\n",
        "for tensor in tensor_details:\n",
        "    if tensor['name'] != 'constants':\n",
        "        print('Name:', tensor['name'])\n",
        "        print('Data type:', tensor['dtype'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1edOQobSfuE",
        "outputId": "d88c7861-38a7-4824-a6f3-f6f00e403508"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: serving_default_dense_10_input:0\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_5/dense_11/MatMul\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_5/dense_10/MatMul1\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: sequential_5/dense_10/MatMul;sequential_5/dense_10/Relu;sequential_5/dense_10/BiasAdd\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: StatefulPartitionedCall:0\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: \n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: \n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: \n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: \n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: \n",
            "Data type: <class 'numpy.int32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integer Quantization - TensorFlow\n",
        "\n",
        "We need a smaple dataset. I would like to try with titanic dataset."
      ],
      "metadata": {
        "id": "8w04oFQBWg1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load dataset.\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
        "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n",
        "y_train = dftrain.pop('survived')\n",
        "y_eval = dfeval.pop('survived')\n",
        "\n",
        "# Use only numeric values\n",
        "dfeval = dfeval.dropna()\n",
        "dfeval_num = dfeval[[\"age\",\"n_siblings_spouses\",\"parch\",\"fare\"]]\n",
        "dftrain = dftrain.dropna()\n",
        "dftrain_num = dftrain[[\"age\",\"n_siblings_spouses\",\"parch\",\"fare\"]]\n",
        "\n",
        "train_data_ = np.array(dftrain_num)\n",
        "train_labels = np.array(y_train)\n",
        "test_data_ = np.array(dfeval_num)\n",
        "test_labels = np.array(y_eval)"
      ],
      "metadata": {
        "id": "5SYiKb3_YkEy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def representative_data_gen():\n",
        "  for i in range(100):\n",
        "    yield [test_data_[i].astype(np.float32)]\n",
        "\n",
        "# Convert model\n",
        "converter_rep = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter_rep.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter_rep.representative_dataset = representative_data_gen\n",
        "tflite_model_quant_rep = converter_rep.convert()\n",
        "\n",
        "open(\"converted_rep_model.tflite\", \"wb\").write(tflite_model_quant_rep) /float(2**10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKcbXKV2WeAa",
        "outputId": "8ff5cd6a-512a-4152-e749-d07b1873f7fa"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "787.28515625"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately the model size is almost the same as the original model."
      ],
      "metadata": {
        "id": "FaAl1R7rhb3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"converted_rep_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get tensor details\n",
        "tensor_details = interpreter.get_tensor_details()\n",
        "\n",
        "# Print data types of parameters\n",
        "for tensor in tensor_details:\n",
        "    if tensor['name'] != 'constants':\n",
        "        print('Name:', tensor['name'])\n",
        "        print('Data type:', tensor['dtype'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_5SqMSofC_S",
        "outputId": "4628ed94-e399-4262-8f55-f13d2c627e52"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: serving_default_dense_10_input:0\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_5/dense_11/MatMul\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_5/dense_10/MatMul1\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_5/dense_10/MatMul;sequential_5/dense_10/Relu;sequential_5/dense_10/BiasAdd\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: StatefulPartitionedCall:0\n",
            "Data type: <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is not expected. Let's use another model and convert."
      ],
      "metadata": {
        "id": "iTcsCvAkggb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "test_images = test_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "# save model - first way\n",
        "model.save('model.h5')\n",
        "\n",
        "# save model - second way\n",
        "saved_model_path = 'saved_model/1'\n",
        "tf.saved_model.save(model, saved_model_path)\n",
        "\n",
        "# Get file size in bytes for a given model - first way\n",
        "TF_model_size = os.path.getsize('model.h5') /float(2**10)\n",
        "print('TF_model_size in KB', TF_model_size)\n",
        "\n",
        "\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
        "    # Model has only one input so each data point has one element.\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "tflite_model_quant = converter.convert()\n",
        "open(\"converted_rep_model.tflite\", \"wb\").write(tflite_model_quant) /float(2**10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raX6kTUDgk0g",
        "outputId": "3b7d97e9-f2ff-4f52-a2d3-1e725d2f1265"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF_model_size in KB 96.4765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.890625"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"converted_rep_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get tensor details\n",
        "tensor_details = interpreter.get_tensor_details()\n",
        "\n",
        "# Print data types of parameters\n",
        "for tensor in tensor_details:\n",
        "    if tensor['name'] != 'constants':\n",
        "        print('Name:', tensor['name'])\n",
        "        print('Data type:', tensor['dtype'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPGU7_TLg7Km",
        "outputId": "a7e1ff88-f124-4c9a-c364-9c9024159488"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: serving_default_input_1:0\n",
            "Data type: <class 'numpy.float32'>\n",
            "Name: sequential_6/reshape/strided_slice/stack\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/reshape/strided_slice/stack_1\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/reshape/Reshape/shape/1\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/reshape/Reshape/shape/3\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/flatten/Const\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/dense_12/MatMul1\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: sequential_6/conv2d/BiasAdd/ReadVariableOp\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/conv2d/Conv2D\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: tfl.quantize\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: sequential_6/reshape/Shape\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/reshape/strided_slice\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/reshape/Reshape/shape\n",
            "Data type: <class 'numpy.int32'>\n",
            "Name: sequential_6/reshape/Reshape\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: sequential_6/conv2d/Relu;sequential_6/conv2d/BiasAdd;sequential_6/conv2d/BiasAdd/ReadVariableOp;sequential_6/conv2d/Conv2D\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: sequential_6/max_pooling2d/MaxPool\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: sequential_6/flatten/Reshape\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: StatefulPartitionedCall:01\n",
            "Data type: <class 'numpy.int8'>\n",
            "Name: StatefulPartitionedCall:0\n",
            "Data type: <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lifrzGUfNuO-"
      },
      "source": [
        "## Linear Quantization - PyTorch\n",
        "\n",
        "It enables the quantized model to maintain performance much closer to the original model by converting from the compressed data type back to the original FP32 data type during inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Torch\n",
        "The original codes: https://github.com/hkproj/quantization-notes/blob/main/post_training_quantization.ipynb"
      ],
      "metadata": {
        "id": "xfmmYsMEHVTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "\n",
        "class DummyTorchModel(nn.Module):\n",
        "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
        "        super(VerySimpleNet,self).__init__()\n",
        "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
        "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = img.view(-1, 28*28)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4h7T3L3JHYAy"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = \"cpu\"\n",
        "original_model = DummyTorchModel().to(device)\n",
        "\n",
        "\n",
        "print('Weights before quantization')\n",
        "print(original_model.linear1.weight)\n",
        "print(original_model.linear1.weight.dtype)\n",
        "\n",
        "print('Size of the model')\n",
        "torch.save(original_model.state_dict(), \"model.p\")\n",
        "print('Size (KB):', os.path.getsize(\"model.p\")/1e3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTaVwy3xHhQa",
        "outputId": "4cbe42f2-dbcc-491c-f6b5-f7c8a2b9ec98"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights before quantization\n",
            "Parameter containing:\n",
            "tensor([[ 1.3020e-02,  3.1752e-02, -4.7959e-03,  ..., -3.6602e-03,\n",
            "          2.1780e-02,  3.3144e-02],\n",
            "        [ 9.8718e-05,  2.7122e-02, -1.9646e-02,  ...,  3.1045e-02,\n",
            "         -2.1462e-02,  3.8348e-03],\n",
            "        [-1.9780e-03,  2.4252e-02, -1.5464e-02,  ...,  1.0473e-03,\n",
            "         -9.3692e-03, -2.6161e-02],\n",
            "        ...,\n",
            "        [ 2.6387e-02,  3.5439e-02, -2.9204e-02,  ..., -2.4509e-02,\n",
            "         -1.6869e-02,  2.8564e-02],\n",
            "        [ 2.7821e-02,  3.4160e-02,  2.7421e-02,  ...,  1.3018e-02,\n",
            "          3.8771e-03,  2.5858e-02],\n",
            "        [-2.7773e-02,  3.5401e-02,  2.2690e-03,  ..., -2.7014e-03,\n",
            "         -2.8694e-02, -2.1597e-02]], requires_grad=True)\n",
            "torch.float32\n",
            "Size of the model before quantization\n",
            "Size (KB): 360.998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvR1wd7AIY3e",
        "outputId": "cd2cd0a5-42ce-4bde-b5c5-71ff6ec6270d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VerySimpleNet(\n",
              "  (linear1): Linear(in_features=784, out_features=100, bias=True)\n",
              "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedModel(nn.Module):\n",
        "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
        "        super(QuantizedVerySimpleNet,self).__init__()\n",
        "        # Define Quant\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
        "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        # Define DeQuant\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = img.view(-1, 28*28)\n",
        "        x = self.quant(x)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yVtwzC6OIEtd"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = \"cpu\"\n",
        "quantized_model = QuantizedModel().to(device)\n",
        "\n",
        "# Copy weights from unquantized model\n",
        "quantized_model.load_state_dict(original_model.state_dict())\n",
        "\n",
        "\n",
        "quantized_model.qconfig = torch.ao.quantization.default_qconfig\n",
        "quantized_model = torch.ao.quantization.prepare(quantized_model) # Insert observers\n",
        "quantized_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CZBlV9PIIXl",
        "outputId": "3ca511c9-02f3-4993-9753-76016ff5e738"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedVerySimpleNet(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.ao.quantization.convert(quantized_model)\n",
        "quantized_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS7vXhNaIkaC",
        "outputId": "d1d09da2-9c81-4ce3-ddf6-e2aec28014e5"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedVerySimpleNet(\n",
              "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
              "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
              "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the weights matrix of the model after quantization\n",
        "print('Weights before quantization')\n",
        "print(quantized_model.linear1.weight())\n",
        "\n",
        "print('Size of the model')\n",
        "torch.save(quantized_model.state_dict(), \"quant_model.p\")\n",
        "print('Size (KB):', os.path.getsize(\"quant_model.p\")/1e3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN4mLJLqIr8q",
        "outputId": "6ff7ba2f-8f04-43ea-dfce-3eb47703c8ef"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights before quantization\n",
            "tensor([[ 0.0129,  0.0317, -0.0048,  ..., -0.0036,  0.0218,  0.0331],\n",
            "        [ 0.0000,  0.0272, -0.0196,  ...,  0.0311, -0.0216,  0.0039],\n",
            "        [-0.0020,  0.0244, -0.0154,  ...,  0.0011, -0.0092, -0.0261],\n",
            "        ...,\n",
            "        [ 0.0263,  0.0356, -0.0291,  ..., -0.0244, -0.0168,  0.0286],\n",
            "        [ 0.0277,  0.0342,  0.0275,  ...,  0.0129,  0.0039,  0.0258],\n",
            "        [-0.0277,  0.0353,  0.0022,  ..., -0.0028, -0.0286, -0.0216]],\n",
            "       size=(100, 784), dtype=torch.qint8,\n",
            "       quantization_scheme=torch.per_tensor_affine, scale=0.00028011034009978175,\n",
            "       zero_point=0)\n",
            "Size of the model before quantization\n",
            "Size (KB): 95.394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original weights: ')\n",
        "print(original_model.linear1.weight)\n",
        "print('')\n",
        "print(f'Dequantized weights: ')\n",
        "print(torch.dequantize(quantized_model.linear1.weight()))\n",
        "print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLE7uGTDJRZv",
        "outputId": "5b04e759-7e87-406d-8e5f-0529bff3fdd2"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weights: \n",
            "Parameter containing:\n",
            "tensor([[ 1.3020e-02,  3.1752e-02, -4.7959e-03,  ..., -3.6602e-03,\n",
            "          2.1780e-02,  3.3144e-02],\n",
            "        [ 9.8718e-05,  2.7122e-02, -1.9646e-02,  ...,  3.1045e-02,\n",
            "         -2.1462e-02,  3.8348e-03],\n",
            "        [-1.9780e-03,  2.4252e-02, -1.5464e-02,  ...,  1.0473e-03,\n",
            "         -9.3692e-03, -2.6161e-02],\n",
            "        ...,\n",
            "        [ 2.6387e-02,  3.5439e-02, -2.9204e-02,  ..., -2.4509e-02,\n",
            "         -1.6869e-02,  2.8564e-02],\n",
            "        [ 2.7821e-02,  3.4160e-02,  2.7421e-02,  ...,  1.3018e-02,\n",
            "          3.8771e-03,  2.5858e-02],\n",
            "        [-2.7773e-02,  3.5401e-02,  2.2690e-03,  ..., -2.7014e-03,\n",
            "         -2.8694e-02, -2.1597e-02]], requires_grad=True)\n",
            "\n",
            "Dequantized weights: \n",
            "tensor([[ 0.0129,  0.0317, -0.0048,  ..., -0.0036,  0.0218,  0.0331],\n",
            "        [ 0.0000,  0.0272, -0.0196,  ...,  0.0311, -0.0216,  0.0039],\n",
            "        [-0.0020,  0.0244, -0.0154,  ...,  0.0011, -0.0092, -0.0261],\n",
            "        ...,\n",
            "        [ 0.0263,  0.0356, -0.0291,  ..., -0.0244, -0.0168,  0.0286],\n",
            "        [ 0.0277,  0.0342,  0.0275,  ...,  0.0129,  0.0039,  0.0258],\n",
            "        [-0.0277,  0.0353,  0.0022,  ..., -0.0028, -0.0286, -0.0216]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Quanto Library"
      ],
      "metadata": {
        "id": "WVd6ru-GHR05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFzK_pMUNOFV",
        "outputId": "592175d5-ccd9-4c38-8e09-fe1cf78ed73b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "\n",
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "base_model.to(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-heVvISPsfL",
        "outputId": "a4600b50-c3ac-4279-f19f-f680c7b32743"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[12092,    13,   619,  1416,   310,   209,   187,    29,    81,    31,\n",
              "           187,    29,    81,    31,   187,    29]])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_text = \"Hello, my name is \"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = base_model.generate(**input_ids, max_new_tokens=10)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "buE9hCW-QRwL",
        "outputId": "affefa72-3452-40c0-b1c8-dc1860f69cbc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello, my name is \\n<p>\\n<p>\\n<'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(outputs[0], skip_special_tkens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgc5wzY2RqRn",
        "outputId": "db268ea5-aba2-499d-f731-baeadc84d91f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0309,  0.0214, -0.0100,  ...,  0.0015,  0.0079,  0.0244],\n",
              "        [-0.0027,  0.0154, -0.0028,  ...,  0.0334,  0.0286, -0.0287],\n",
              "        [ 0.0411,  0.0161, -0.0055,  ..., -0.0134, -0.0373, -0.0166],\n",
              "        ...,\n",
              "        [-0.0063,  0.0312, -0.0121,  ...,  0.0329, -0.0504,  0.0386],\n",
              "        [-0.0065,  0.0047,  0.0069,  ...,  0.0031, -0.0621, -0.0452],\n",
              "        [ 0.0078, -0.0311,  0.0021,  ...,  0.0286,  0.0178, -0.0465]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.gpt_neox.layers[0].attention.dense.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T8yoOO6QfAr",
        "outputId": "a6a400c4-7cff-44c7-ad16-36708789b9db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: quanto in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from quanto) (2.2.1+cu121)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from quanto) (1.11.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from quanto) (1.25.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from quanto) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->quanto) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2.0->quanto) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0->quanto) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install quanto\n",
        "import quanto\n",
        "from quanto import quantize, freeze\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp2bLLXJRe-Z"
      },
      "outputs": [],
      "source": [
        "quanto.quantize(base_model, weights=quanto.qint8, activations=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elhruxEjRer7",
        "outputId": "5b958d04-48f5-4da1-9f78-6670c374eb05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0309,  0.0214, -0.0100,  ...,  0.0015,  0.0079,  0.0244],\n",
              "        [-0.0027,  0.0154, -0.0028,  ...,  0.0334,  0.0286, -0.0287],\n",
              "        [ 0.0411,  0.0161, -0.0055,  ..., -0.0134, -0.0373, -0.0166],\n",
              "        ...,\n",
              "        [-0.0063,  0.0312, -0.0121,  ...,  0.0329, -0.0504,  0.0386],\n",
              "        [-0.0065,  0.0047,  0.0069,  ...,  0.0031, -0.0621, -0.0452],\n",
              "        [ 0.0078, -0.0311,  0.0021,  ...,  0.0286,  0.0178, -0.0465]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.gpt_neox.layers[0].attention.dense.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OefKuP3yUHUu"
      },
      "source": [
        "Nothing changed???"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n4p3e8UUDhw",
        "outputId": "f8057e39-3719-49b0-c55f-777f151d3561"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QTensor(tensor([[  53,   37,  -17,  ...,    3,   14,   42],\n",
              "        [  -4,   21,   -4,  ...,   46,   39,  -39],\n",
              "        [  90,   35,  -12,  ...,  -29,  -81,  -36],\n",
              "        ...,\n",
              "        [ -12,   61,  -24,  ...,   64,  -98,   75],\n",
              "        [ -11,    8,   11,  ...,    5, -102,  -74],\n",
              "        [  15,  -61,    4,  ...,   56,   35,  -91]], dtype=torch.int8), scale=tensor([[0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0010],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0010],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0008],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0008],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0008],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0028],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0012],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0008],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0008],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0008],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0008],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0008],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0008],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0012],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0027],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0008],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0008],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0013],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0004],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0007],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0006],\n",
              "        [0.0008],\n",
              "        [0.0005],\n",
              "        [0.0005],\n",
              "        [0.0006],\n",
              "        [0.0005]]), public_dtype=torch.float32, requires_grad=True)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freeze(base_model)\n",
        "base_model.gpt_neox.layers[0].attention.dense.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBmlYSq8UdAd",
        "outputId": "d396f76e-a775-4370-b9f2-2bafafce1735"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/quanto/library/ops.py:52: UserWarning: An exception was raised while calling the optimized kernel for quanto::dqmm: /root/.cache/torch_extensions/py310_cu121/quanto_cpp/quanto_cpp.so: cannot open shared object file: No such file or directory Falling back to default implementation.\n",
            "  warnings.warn(message + \" Falling back to default implementation.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[12092,    13,   619,  1416,   310,   209,   187,    29,    67,  5651,\n",
              "            14,    77, 48013,    31, 14260,   187]])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_text = \"Hello, my name is \"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = base_model.generate(**input_ids, max_new_tokens=10)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1-opgy52VCWQ",
        "outputId": "d10174f7-a7a5-4fb3-bef6-b0d169fe64e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello, my name is \\n<brian-laptop> hi\\n'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(outputs[0], skip_special_tkens=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-UmwRn1ErQCC",
        "nz64HNKpA7Wj",
        "REvVynrIA_K2",
        "8K079pFgSMC-",
        "8w04oFQBWg1V",
        "WVd6ru-GHR05"
      ],
      "authorship_tag": "ABX9TyMLw3TsNhTaOxZ0hyO04Y+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}